{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAB_1_PART_2_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZlc0LhTW-Rw",
        "outputId": "efa88f09-2d6e-4af3-c453-811aadd31423"
      },
      "source": [
        "!pip install kaggle\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=d992ce62783bf542819c4d3d34752babdbb10ac1f257a18c191ad1e98ef5ad76\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "j0IuQiK1X6-6",
        "outputId": "1d9ad009-8b05-4094-d680-8ac70216a1b9"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-70f98c18-656b-471d-9622-feb63ad48068\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-70f98c18-656b-471d-9622-feb63ad48068\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"dhruvinanilshah\",\"key\":\"e7f1c44d72b99cf94b400854cc49750b\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bfz3kj5X6z9"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecsrp1fLX6x7",
        "outputId": "059a161d-530d-42d7-d815-087cbc3f45cf"
      },
      "source": [
        "!kaggle competitions download -c sjsu-cmpe-258-fa21-lab1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sjsu-cmpe-258-fa21-lab1.zip to /content\n",
            " 72% 30.0M/41.4M [00:00<00:00, 161MB/s]\n",
            "100% 41.4M/41.4M [00:00<00:00, 156MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lapuKtJX6vf",
        "outputId": "f02c706c-3960-4efd-9b6b-ddaae81907ef"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"sjsu-cmpe-258-fa21-lab1.zip\"\n",
        "\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8I16opgYYYZ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import itertools\n",
        "import keras\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
        "from keras.models import Sequential \n",
        "from keras import optimizers\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Dropout, Flatten, Dense, BatchNormalization, Activation\n",
        "from keras import applications \n",
        "from keras.utils.np_utils import to_categorical \n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "import math \n",
        "import datetime\n",
        "import time\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.constraints import maxnorm\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "3tzoPD8XY3Wq",
        "outputId": "de8008e9-4607-418a-a755-8ae4d8b37fc7"
      },
      "source": [
        "train_data = pd.read_csv('/content/train_data.csv',header=None)\n",
        "train_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>3032</th>\n",
              "      <th>3033</th>\n",
              "      <th>3034</th>\n",
              "      <th>3035</th>\n",
              "      <th>3036</th>\n",
              "      <th>3037</th>\n",
              "      <th>3038</th>\n",
              "      <th>3039</th>\n",
              "      <th>3040</th>\n",
              "      <th>3041</th>\n",
              "      <th>3042</th>\n",
              "      <th>3043</th>\n",
              "      <th>3044</th>\n",
              "      <th>3045</th>\n",
              "      <th>3046</th>\n",
              "      <th>3047</th>\n",
              "      <th>3048</th>\n",
              "      <th>3049</th>\n",
              "      <th>3050</th>\n",
              "      <th>3051</th>\n",
              "      <th>3052</th>\n",
              "      <th>3053</th>\n",
              "      <th>3054</th>\n",
              "      <th>3055</th>\n",
              "      <th>3056</th>\n",
              "      <th>3057</th>\n",
              "      <th>3058</th>\n",
              "      <th>3059</th>\n",
              "      <th>3060</th>\n",
              "      <th>3061</th>\n",
              "      <th>3062</th>\n",
              "      <th>3063</th>\n",
              "      <th>3064</th>\n",
              "      <th>3065</th>\n",
              "      <th>3066</th>\n",
              "      <th>3067</th>\n",
              "      <th>3068</th>\n",
              "      <th>3069</th>\n",
              "      <th>3070</th>\n",
              "      <th>3071</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>55</td>\n",
              "      <td>68</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>76</td>\n",
              "      <td>63</td>\n",
              "      <td>67</td>\n",
              "      <td>86</td>\n",
              "      <td>56</td>\n",
              "      <td>80</td>\n",
              "      <td>102</td>\n",
              "      <td>57</td>\n",
              "      <td>97</td>\n",
              "      <td>118</td>\n",
              "      <td>76</td>\n",
              "      <td>85</td>\n",
              "      <td>107</td>\n",
              "      <td>62</td>\n",
              "      <td>74</td>\n",
              "      <td>97</td>\n",
              "      <td>47</td>\n",
              "      <td>94</td>\n",
              "      <td>118</td>\n",
              "      <td>63</td>\n",
              "      <td>98</td>\n",
              "      <td>121</td>\n",
              "      <td>67</td>\n",
              "      <td>84</td>\n",
              "      <td>102</td>\n",
              "      <td>59</td>\n",
              "      <td>109</td>\n",
              "      <td>128</td>\n",
              "      <td>77</td>\n",
              "      <td>123</td>\n",
              "      <td>145</td>\n",
              "      <td>80</td>\n",
              "      <td>120</td>\n",
              "      <td>...</td>\n",
              "      <td>158</td>\n",
              "      <td>179</td>\n",
              "      <td>176</td>\n",
              "      <td>147</td>\n",
              "      <td>180</td>\n",
              "      <td>180</td>\n",
              "      <td>149</td>\n",
              "      <td>176</td>\n",
              "      <td>177</td>\n",
              "      <td>147</td>\n",
              "      <td>178</td>\n",
              "      <td>180</td>\n",
              "      <td>139</td>\n",
              "      <td>171</td>\n",
              "      <td>175</td>\n",
              "      <td>124</td>\n",
              "      <td>174</td>\n",
              "      <td>178</td>\n",
              "      <td>130</td>\n",
              "      <td>173</td>\n",
              "      <td>175</td>\n",
              "      <td>140</td>\n",
              "      <td>165</td>\n",
              "      <td>170</td>\n",
              "      <td>132</td>\n",
              "      <td>171</td>\n",
              "      <td>178</td>\n",
              "      <td>134</td>\n",
              "      <td>169</td>\n",
              "      <td>176</td>\n",
              "      <td>133</td>\n",
              "      <td>170</td>\n",
              "      <td>177</td>\n",
              "      <td>133</td>\n",
              "      <td>170</td>\n",
              "      <td>177</td>\n",
              "      <td>133</td>\n",
              "      <td>164</td>\n",
              "      <td>168</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>74</td>\n",
              "      <td>64</td>\n",
              "      <td>62</td>\n",
              "      <td>73</td>\n",
              "      <td>63</td>\n",
              "      <td>61</td>\n",
              "      <td>77</td>\n",
              "      <td>67</td>\n",
              "      <td>65</td>\n",
              "      <td>78</td>\n",
              "      <td>68</td>\n",
              "      <td>65</td>\n",
              "      <td>79</td>\n",
              "      <td>69</td>\n",
              "      <td>64</td>\n",
              "      <td>78</td>\n",
              "      <td>68</td>\n",
              "      <td>64</td>\n",
              "      <td>78</td>\n",
              "      <td>69</td>\n",
              "      <td>64</td>\n",
              "      <td>79</td>\n",
              "      <td>67</td>\n",
              "      <td>63</td>\n",
              "      <td>80</td>\n",
              "      <td>62</td>\n",
              "      <td>60</td>\n",
              "      <td>80</td>\n",
              "      <td>63</td>\n",
              "      <td>60</td>\n",
              "      <td>81</td>\n",
              "      <td>64</td>\n",
              "      <td>62</td>\n",
              "      <td>85</td>\n",
              "      <td>69</td>\n",
              "      <td>66</td>\n",
              "      <td>85</td>\n",
              "      <td>70</td>\n",
              "      <td>67</td>\n",
              "      <td>83</td>\n",
              "      <td>...</td>\n",
              "      <td>153</td>\n",
              "      <td>147</td>\n",
              "      <td>146</td>\n",
              "      <td>148</td>\n",
              "      <td>146</td>\n",
              "      <td>145</td>\n",
              "      <td>147</td>\n",
              "      <td>149</td>\n",
              "      <td>149</td>\n",
              "      <td>151</td>\n",
              "      <td>156</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>161</td>\n",
              "      <td>161</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>167</td>\n",
              "      <td>169</td>\n",
              "      <td>167</td>\n",
              "      <td>165</td>\n",
              "      <td>168</td>\n",
              "      <td>170</td>\n",
              "      <td>168</td>\n",
              "      <td>171</td>\n",
              "      <td>174</td>\n",
              "      <td>172</td>\n",
              "      <td>175</td>\n",
              "      <td>166</td>\n",
              "      <td>164</td>\n",
              "      <td>167</td>\n",
              "      <td>163</td>\n",
              "      <td>161</td>\n",
              "      <td>164</td>\n",
              "      <td>162</td>\n",
              "      <td>160</td>\n",
              "      <td>163</td>\n",
              "      <td>162</td>\n",
              "      <td>160</td>\n",
              "      <td>163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70</td>\n",
              "      <td>73</td>\n",
              "      <td>87</td>\n",
              "      <td>71</td>\n",
              "      <td>74</td>\n",
              "      <td>88</td>\n",
              "      <td>71</td>\n",
              "      <td>75</td>\n",
              "      <td>88</td>\n",
              "      <td>73</td>\n",
              "      <td>76</td>\n",
              "      <td>90</td>\n",
              "      <td>74</td>\n",
              "      <td>78</td>\n",
              "      <td>91</td>\n",
              "      <td>76</td>\n",
              "      <td>79</td>\n",
              "      <td>93</td>\n",
              "      <td>77</td>\n",
              "      <td>80</td>\n",
              "      <td>93</td>\n",
              "      <td>78</td>\n",
              "      <td>80</td>\n",
              "      <td>94</td>\n",
              "      <td>77</td>\n",
              "      <td>82</td>\n",
              "      <td>99</td>\n",
              "      <td>77</td>\n",
              "      <td>86</td>\n",
              "      <td>108</td>\n",
              "      <td>80</td>\n",
              "      <td>93</td>\n",
              "      <td>120</td>\n",
              "      <td>85</td>\n",
              "      <td>104</td>\n",
              "      <td>136</td>\n",
              "      <td>90</td>\n",
              "      <td>112</td>\n",
              "      <td>148</td>\n",
              "      <td>98</td>\n",
              "      <td>...</td>\n",
              "      <td>44</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>42</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>6</td>\n",
              "      <td>27</td>\n",
              "      <td>42</td>\n",
              "      <td>6</td>\n",
              "      <td>28</td>\n",
              "      <td>43</td>\n",
              "      <td>6</td>\n",
              "      <td>27</td>\n",
              "      <td>42</td>\n",
              "      <td>6</td>\n",
              "      <td>27</td>\n",
              "      <td>42</td>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "      <td>43</td>\n",
              "      <td>6</td>\n",
              "      <td>26</td>\n",
              "      <td>41</td>\n",
              "      <td>5</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>5</td>\n",
              "      <td>24</td>\n",
              "      <td>38</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>69</td>\n",
              "      <td>70</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>102</td>\n",
              "      <td>5</td>\n",
              "      <td>96</td>\n",
              "      <td>106</td>\n",
              "      <td>1</td>\n",
              "      <td>94</td>\n",
              "      <td>96</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>86</td>\n",
              "      <td>5</td>\n",
              "      <td>82</td>\n",
              "      <td>79</td>\n",
              "      <td>5</td>\n",
              "      <td>75</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>70</td>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>90</td>\n",
              "      <td>84</td>\n",
              "      <td>9</td>\n",
              "      <td>94</td>\n",
              "      <td>91</td>\n",
              "      <td>15</td>\n",
              "      <td>34</td>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>101</td>\n",
              "      <td>84</td>\n",
              "      <td>7</td>\n",
              "      <td>128</td>\n",
              "      <td>109</td>\n",
              "      <td>5</td>\n",
              "      <td>136</td>\n",
              "      <td>114</td>\n",
              "      <td>4</td>\n",
              "      <td>144</td>\n",
              "      <td>117</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "      <td>116</td>\n",
              "      <td>5</td>\n",
              "      <td>153</td>\n",
              "      <td>112</td>\n",
              "      <td>7</td>\n",
              "      <td>155</td>\n",
              "      <td>96</td>\n",
              "      <td>5</td>\n",
              "      <td>141</td>\n",
              "      <td>78</td>\n",
              "      <td>9</td>\n",
              "      <td>108</td>\n",
              "      <td>62</td>\n",
              "      <td>5</td>\n",
              "      <td>94</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>84</td>\n",
              "      <td>41</td>\n",
              "      <td>2</td>\n",
              "      <td>97</td>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>125</td>\n",
              "      <td>119</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19</td>\n",
              "      <td>64</td>\n",
              "      <td>102</td>\n",
              "      <td>23</td>\n",
              "      <td>75</td>\n",
              "      <td>116</td>\n",
              "      <td>12</td>\n",
              "      <td>81</td>\n",
              "      <td>122</td>\n",
              "      <td>23</td>\n",
              "      <td>109</td>\n",
              "      <td>147</td>\n",
              "      <td>25</td>\n",
              "      <td>117</td>\n",
              "      <td>151</td>\n",
              "      <td>12</td>\n",
              "      <td>101</td>\n",
              "      <td>141</td>\n",
              "      <td>9</td>\n",
              "      <td>95</td>\n",
              "      <td>143</td>\n",
              "      <td>10</td>\n",
              "      <td>95</td>\n",
              "      <td>139</td>\n",
              "      <td>16</td>\n",
              "      <td>101</td>\n",
              "      <td>136</td>\n",
              "      <td>24</td>\n",
              "      <td>112</td>\n",
              "      <td>144</td>\n",
              "      <td>23</td>\n",
              "      <td>108</td>\n",
              "      <td>142</td>\n",
              "      <td>16</td>\n",
              "      <td>96</td>\n",
              "      <td>133</td>\n",
              "      <td>39</td>\n",
              "      <td>112</td>\n",
              "      <td>152</td>\n",
              "      <td>31</td>\n",
              "      <td>...</td>\n",
              "      <td>134</td>\n",
              "      <td>113</td>\n",
              "      <td>149</td>\n",
              "      <td>136</td>\n",
              "      <td>114</td>\n",
              "      <td>152</td>\n",
              "      <td>140</td>\n",
              "      <td>119</td>\n",
              "      <td>159</td>\n",
              "      <td>150</td>\n",
              "      <td>122</td>\n",
              "      <td>164</td>\n",
              "      <td>156</td>\n",
              "      <td>133</td>\n",
              "      <td>176</td>\n",
              "      <td>171</td>\n",
              "      <td>147</td>\n",
              "      <td>188</td>\n",
              "      <td>189</td>\n",
              "      <td>154</td>\n",
              "      <td>196</td>\n",
              "      <td>200</td>\n",
              "      <td>157</td>\n",
              "      <td>202</td>\n",
              "      <td>207</td>\n",
              "      <td>148</td>\n",
              "      <td>195</td>\n",
              "      <td>203</td>\n",
              "      <td>147</td>\n",
              "      <td>197</td>\n",
              "      <td>205</td>\n",
              "      <td>131</td>\n",
              "      <td>187</td>\n",
              "      <td>193</td>\n",
              "      <td>116</td>\n",
              "      <td>176</td>\n",
              "      <td>183</td>\n",
              "      <td>103</td>\n",
              "      <td>168</td>\n",
              "      <td>177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>30</td>\n",
              "      <td>25</td>\n",
              "      <td>16</td>\n",
              "      <td>52</td>\n",
              "      <td>42</td>\n",
              "      <td>38</td>\n",
              "      <td>55</td>\n",
              "      <td>48</td>\n",
              "      <td>47</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>28</td>\n",
              "      <td>35</td>\n",
              "      <td>25</td>\n",
              "      <td>15</td>\n",
              "      <td>23</td>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "      <td>44</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>33</td>\n",
              "      <td>25</td>\n",
              "      <td>20</td>\n",
              "      <td>...</td>\n",
              "      <td>34</td>\n",
              "      <td>69</td>\n",
              "      <td>73</td>\n",
              "      <td>43</td>\n",
              "      <td>25</td>\n",
              "      <td>23</td>\n",
              "      <td>8</td>\n",
              "      <td>39</td>\n",
              "      <td>30</td>\n",
              "      <td>23</td>\n",
              "      <td>60</td>\n",
              "      <td>51</td>\n",
              "      <td>48</td>\n",
              "      <td>57</td>\n",
              "      <td>50</td>\n",
              "      <td>49</td>\n",
              "      <td>29</td>\n",
              "      <td>24</td>\n",
              "      <td>19</td>\n",
              "      <td>24</td>\n",
              "      <td>23</td>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>13</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>21</td>\n",
              "      <td>19</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>104</td>\n",
              "      <td>82</td>\n",
              "      <td>106</td>\n",
              "      <td>103</td>\n",
              "      <td>76</td>\n",
              "      <td>100</td>\n",
              "      <td>110</td>\n",
              "      <td>83</td>\n",
              "      <td>87</td>\n",
              "      <td>85</td>\n",
              "      <td>67</td>\n",
              "      <td>65</td>\n",
              "      <td>75</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "      <td>56</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>53</td>\n",
              "      <td>25</td>\n",
              "      <td>23</td>\n",
              "      <td>41</td>\n",
              "      <td>16</td>\n",
              "      <td>14</td>\n",
              "      <td>45</td>\n",
              "      <td>26</td>\n",
              "      <td>24</td>\n",
              "      <td>53</td>\n",
              "      <td>33</td>\n",
              "      <td>32</td>\n",
              "      <td>45</td>\n",
              "      <td>26</td>\n",
              "      <td>24</td>\n",
              "      <td>54</td>\n",
              "      <td>34</td>\n",
              "      <td>35</td>\n",
              "      <td>81</td>\n",
              "      <td>60</td>\n",
              "      <td>68</td>\n",
              "      <td>116</td>\n",
              "      <td>...</td>\n",
              "      <td>225</td>\n",
              "      <td>202</td>\n",
              "      <td>180</td>\n",
              "      <td>200</td>\n",
              "      <td>172</td>\n",
              "      <td>157</td>\n",
              "      <td>176</td>\n",
              "      <td>221</td>\n",
              "      <td>211</td>\n",
              "      <td>229</td>\n",
              "      <td>241</td>\n",
              "      <td>236</td>\n",
              "      <td>253</td>\n",
              "      <td>241</td>\n",
              "      <td>238</td>\n",
              "      <td>255</td>\n",
              "      <td>239</td>\n",
              "      <td>237</td>\n",
              "      <td>249</td>\n",
              "      <td>244</td>\n",
              "      <td>239</td>\n",
              "      <td>250</td>\n",
              "      <td>245</td>\n",
              "      <td>241</td>\n",
              "      <td>252</td>\n",
              "      <td>246</td>\n",
              "      <td>242</td>\n",
              "      <td>251</td>\n",
              "      <td>193</td>\n",
              "      <td>187</td>\n",
              "      <td>202</td>\n",
              "      <td>142</td>\n",
              "      <td>132</td>\n",
              "      <td>154</td>\n",
              "      <td>212</td>\n",
              "      <td>202</td>\n",
              "      <td>224</td>\n",
              "      <td>227</td>\n",
              "      <td>217</td>\n",
              "      <td>239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>204</td>\n",
              "      <td>253</td>\n",
              "      <td>255</td>\n",
              "      <td>203</td>\n",
              "      <td>254</td>\n",
              "      <td>255</td>\n",
              "      <td>209</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>206</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>204</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>204</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>204</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>204</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>204</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>204</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>204</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>204</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>204</td>\n",
              "      <td>255</td>\n",
              "      <td>...</td>\n",
              "      <td>135</td>\n",
              "      <td>148</td>\n",
              "      <td>151</td>\n",
              "      <td>115</td>\n",
              "      <td>151</td>\n",
              "      <td>158</td>\n",
              "      <td>122</td>\n",
              "      <td>147</td>\n",
              "      <td>153</td>\n",
              "      <td>118</td>\n",
              "      <td>140</td>\n",
              "      <td>147</td>\n",
              "      <td>110</td>\n",
              "      <td>153</td>\n",
              "      <td>160</td>\n",
              "      <td>123</td>\n",
              "      <td>157</td>\n",
              "      <td>162</td>\n",
              "      <td>126</td>\n",
              "      <td>141</td>\n",
              "      <td>147</td>\n",
              "      <td>110</td>\n",
              "      <td>147</td>\n",
              "      <td>154</td>\n",
              "      <td>116</td>\n",
              "      <td>148</td>\n",
              "      <td>156</td>\n",
              "      <td>118</td>\n",
              "      <td>153</td>\n",
              "      <td>159</td>\n",
              "      <td>121</td>\n",
              "      <td>206</td>\n",
              "      <td>207</td>\n",
              "      <td>164</td>\n",
              "      <td>246</td>\n",
              "      <td>250</td>\n",
              "      <td>199</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>245</td>\n",
              "      <td>248</td>\n",
              "      <td>247</td>\n",
              "      <td>227</td>\n",
              "      <td>230</td>\n",
              "      <td>229</td>\n",
              "      <td>239</td>\n",
              "      <td>242</td>\n",
              "      <td>241</td>\n",
              "      <td>248</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>...</td>\n",
              "      <td>254</td>\n",
              "      <td>255</td>\n",
              "      <td>252</td>\n",
              "      <td>255</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>255</td>\n",
              "      <td>238</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>200</td>\n",
              "      <td>243</td>\n",
              "      <td>243</td>\n",
              "      <td>153</td>\n",
              "      <td>232</td>\n",
              "      <td>236</td>\n",
              "      <td>195</td>\n",
              "      <td>242</td>\n",
              "      <td>246</td>\n",
              "      <td>249</td>\n",
              "      <td>253</td>\n",
              "      <td>255</td>\n",
              "      <td>254</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>81</td>\n",
              "      <td>120</td>\n",
              "      <td>162</td>\n",
              "      <td>83</td>\n",
              "      <td>121</td>\n",
              "      <td>163</td>\n",
              "      <td>84</td>\n",
              "      <td>123</td>\n",
              "      <td>165</td>\n",
              "      <td>86</td>\n",
              "      <td>125</td>\n",
              "      <td>167</td>\n",
              "      <td>88</td>\n",
              "      <td>126</td>\n",
              "      <td>168</td>\n",
              "      <td>90</td>\n",
              "      <td>128</td>\n",
              "      <td>168</td>\n",
              "      <td>92</td>\n",
              "      <td>130</td>\n",
              "      <td>170</td>\n",
              "      <td>93</td>\n",
              "      <td>131</td>\n",
              "      <td>171</td>\n",
              "      <td>95</td>\n",
              "      <td>132</td>\n",
              "      <td>173</td>\n",
              "      <td>97</td>\n",
              "      <td>134</td>\n",
              "      <td>174</td>\n",
              "      <td>98</td>\n",
              "      <td>136</td>\n",
              "      <td>175</td>\n",
              "      <td>99</td>\n",
              "      <td>137</td>\n",
              "      <td>175</td>\n",
              "      <td>100</td>\n",
              "      <td>139</td>\n",
              "      <td>174</td>\n",
              "      <td>100</td>\n",
              "      <td>...</td>\n",
              "      <td>82</td>\n",
              "      <td>88</td>\n",
              "      <td>89</td>\n",
              "      <td>84</td>\n",
              "      <td>86</td>\n",
              "      <td>86</td>\n",
              "      <td>81</td>\n",
              "      <td>87</td>\n",
              "      <td>84</td>\n",
              "      <td>80</td>\n",
              "      <td>76</td>\n",
              "      <td>78</td>\n",
              "      <td>80</td>\n",
              "      <td>52</td>\n",
              "      <td>59</td>\n",
              "      <td>65</td>\n",
              "      <td>87</td>\n",
              "      <td>91</td>\n",
              "      <td>95</td>\n",
              "      <td>137</td>\n",
              "      <td>135</td>\n",
              "      <td>133</td>\n",
              "      <td>150</td>\n",
              "      <td>147</td>\n",
              "      <td>144</td>\n",
              "      <td>145</td>\n",
              "      <td>144</td>\n",
              "      <td>142</td>\n",
              "      <td>133</td>\n",
              "      <td>132</td>\n",
              "      <td>130</td>\n",
              "      <td>134</td>\n",
              "      <td>133</td>\n",
              "      <td>131</td>\n",
              "      <td>144</td>\n",
              "      <td>143</td>\n",
              "      <td>141</td>\n",
              "      <td>137</td>\n",
              "      <td>136</td>\n",
              "      <td>134</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3072 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0     1     2     3     4     5     ...  3066  3067  3068  3069  3070  3071\n",
              "0       52    63    45    55    68    48  ...   170   177   133   164   168   128\n",
              "1       74    64    62    73    63    61  ...   162   160   163   162   160   163\n",
              "2       70    73    87    71    74    88  ...     2    21    33     1    20    33\n",
              "3       69    70     5    94   102     5  ...    97    70     3   125   119     3\n",
              "4       19    64   102    23    75   116  ...   116   176   183   103   168   177\n",
              "...    ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
              "9995    30    25    16    52    42    38  ...    20    20    12    21    19    11\n",
              "9996   104    82   106   103    76   100  ...   212   202   224   227   217   239\n",
              "9997   255   255   204   253   255   203  ...   246   250   199   255   255   206\n",
              "9998   255   255   255   250   251   251  ...   254   254   254   255   255   255\n",
              "9999    81   120   162    83   121   163  ...   144   143   141   137   136   134\n",
              "\n",
              "[10000 rows x 3072 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5zKMKR0rOQ0",
        "outputId": "c499219e-a562-4d9b-c89b-fb1229034520"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "3Ra23OySZVvr",
        "outputId": "1d1554a4-dee9-4de9-8896-f2c47e803477"
      },
      "source": [
        "test_data = pd.read_csv('/content/test_data.csv',header = None)\n",
        "test_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>3032</th>\n",
              "      <th>3033</th>\n",
              "      <th>3034</th>\n",
              "      <th>3035</th>\n",
              "      <th>3036</th>\n",
              "      <th>3037</th>\n",
              "      <th>3038</th>\n",
              "      <th>3039</th>\n",
              "      <th>3040</th>\n",
              "      <th>3041</th>\n",
              "      <th>3042</th>\n",
              "      <th>3043</th>\n",
              "      <th>3044</th>\n",
              "      <th>3045</th>\n",
              "      <th>3046</th>\n",
              "      <th>3047</th>\n",
              "      <th>3048</th>\n",
              "      <th>3049</th>\n",
              "      <th>3050</th>\n",
              "      <th>3051</th>\n",
              "      <th>3052</th>\n",
              "      <th>3053</th>\n",
              "      <th>3054</th>\n",
              "      <th>3055</th>\n",
              "      <th>3056</th>\n",
              "      <th>3057</th>\n",
              "      <th>3058</th>\n",
              "      <th>3059</th>\n",
              "      <th>3060</th>\n",
              "      <th>3061</th>\n",
              "      <th>3062</th>\n",
              "      <th>3063</th>\n",
              "      <th>3064</th>\n",
              "      <th>3065</th>\n",
              "      <th>3066</th>\n",
              "      <th>3067</th>\n",
              "      <th>3068</th>\n",
              "      <th>3069</th>\n",
              "      <th>3070</th>\n",
              "      <th>3071</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>28</td>\n",
              "      <td>6</td>\n",
              "      <td>83</td>\n",
              "      <td>41</td>\n",
              "      <td>17</td>\n",
              "      <td>58</td>\n",
              "      <td>29</td>\n",
              "      <td>9</td>\n",
              "      <td>28</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>15</td>\n",
              "      <td>7</td>\n",
              "      <td>27</td>\n",
              "      <td>16</td>\n",
              "      <td>5</td>\n",
              "      <td>43</td>\n",
              "      <td>26</td>\n",
              "      <td>12</td>\n",
              "      <td>59</td>\n",
              "      <td>41</td>\n",
              "      <td>23</td>\n",
              "      <td>56</td>\n",
              "      <td>52</td>\n",
              "      <td>14</td>\n",
              "      <td>79</td>\n",
              "      <td>100</td>\n",
              "      <td>29</td>\n",
              "      <td>82</td>\n",
              "      <td>116</td>\n",
              "      <td>29</td>\n",
              "      <td>83</td>\n",
              "      <td>117</td>\n",
              "      <td>31</td>\n",
              "      <td>87</td>\n",
              "      <td>...</td>\n",
              "      <td>19</td>\n",
              "      <td>62</td>\n",
              "      <td>102</td>\n",
              "      <td>12</td>\n",
              "      <td>54</td>\n",
              "      <td>100</td>\n",
              "      <td>4</td>\n",
              "      <td>52</td>\n",
              "      <td>99</td>\n",
              "      <td>3</td>\n",
              "      <td>51</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>54</td>\n",
              "      <td>105</td>\n",
              "      <td>3</td>\n",
              "      <td>57</td>\n",
              "      <td>104</td>\n",
              "      <td>5</td>\n",
              "      <td>55</td>\n",
              "      <td>95</td>\n",
              "      <td>8</td>\n",
              "      <td>55</td>\n",
              "      <td>87</td>\n",
              "      <td>14</td>\n",
              "      <td>46</td>\n",
              "      <td>67</td>\n",
              "      <td>15</td>\n",
              "      <td>18</td>\n",
              "      <td>26</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>18</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34</td>\n",
              "      <td>37</td>\n",
              "      <td>42</td>\n",
              "      <td>34</td>\n",
              "      <td>37</td>\n",
              "      <td>42</td>\n",
              "      <td>33</td>\n",
              "      <td>36</td>\n",
              "      <td>41</td>\n",
              "      <td>31</td>\n",
              "      <td>35</td>\n",
              "      <td>37</td>\n",
              "      <td>31</td>\n",
              "      <td>35</td>\n",
              "      <td>36</td>\n",
              "      <td>31</td>\n",
              "      <td>35</td>\n",
              "      <td>38</td>\n",
              "      <td>31</td>\n",
              "      <td>35</td>\n",
              "      <td>37</td>\n",
              "      <td>31</td>\n",
              "      <td>36</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>35</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>35</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>35</td>\n",
              "      <td>34</td>\n",
              "      <td>31</td>\n",
              "      <td>35</td>\n",
              "      <td>34</td>\n",
              "      <td>30</td>\n",
              "      <td>34</td>\n",
              "      <td>33</td>\n",
              "      <td>29</td>\n",
              "      <td>...</td>\n",
              "      <td>187</td>\n",
              "      <td>182</td>\n",
              "      <td>186</td>\n",
              "      <td>185</td>\n",
              "      <td>182</td>\n",
              "      <td>186</td>\n",
              "      <td>185</td>\n",
              "      <td>183</td>\n",
              "      <td>187</td>\n",
              "      <td>186</td>\n",
              "      <td>182</td>\n",
              "      <td>187</td>\n",
              "      <td>183</td>\n",
              "      <td>183</td>\n",
              "      <td>188</td>\n",
              "      <td>183</td>\n",
              "      <td>185</td>\n",
              "      <td>189</td>\n",
              "      <td>188</td>\n",
              "      <td>185</td>\n",
              "      <td>189</td>\n",
              "      <td>188</td>\n",
              "      <td>183</td>\n",
              "      <td>187</td>\n",
              "      <td>186</td>\n",
              "      <td>180</td>\n",
              "      <td>184</td>\n",
              "      <td>183</td>\n",
              "      <td>181</td>\n",
              "      <td>185</td>\n",
              "      <td>184</td>\n",
              "      <td>182</td>\n",
              "      <td>186</td>\n",
              "      <td>185</td>\n",
              "      <td>179</td>\n",
              "      <td>183</td>\n",
              "      <td>182</td>\n",
              "      <td>178</td>\n",
              "      <td>182</td>\n",
              "      <td>181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>209</td>\n",
              "      <td>227</td>\n",
              "      <td>247</td>\n",
              "      <td>197</td>\n",
              "      <td>224</td>\n",
              "      <td>244</td>\n",
              "      <td>189</td>\n",
              "      <td>228</td>\n",
              "      <td>252</td>\n",
              "      <td>187</td>\n",
              "      <td>222</td>\n",
              "      <td>247</td>\n",
              "      <td>191</td>\n",
              "      <td>228</td>\n",
              "      <td>254</td>\n",
              "      <td>189</td>\n",
              "      <td>228</td>\n",
              "      <td>254</td>\n",
              "      <td>188</td>\n",
              "      <td>229</td>\n",
              "      <td>253</td>\n",
              "      <td>188</td>\n",
              "      <td>229</td>\n",
              "      <td>253</td>\n",
              "      <td>188</td>\n",
              "      <td>226</td>\n",
              "      <td>254</td>\n",
              "      <td>189</td>\n",
              "      <td>225</td>\n",
              "      <td>254</td>\n",
              "      <td>189</td>\n",
              "      <td>223</td>\n",
              "      <td>252</td>\n",
              "      <td>187</td>\n",
              "      <td>221</td>\n",
              "      <td>249</td>\n",
              "      <td>185</td>\n",
              "      <td>219</td>\n",
              "      <td>247</td>\n",
              "      <td>182</td>\n",
              "      <td>...</td>\n",
              "      <td>70</td>\n",
              "      <td>135</td>\n",
              "      <td>164</td>\n",
              "      <td>72</td>\n",
              "      <td>133</td>\n",
              "      <td>161</td>\n",
              "      <td>74</td>\n",
              "      <td>136</td>\n",
              "      <td>163</td>\n",
              "      <td>82</td>\n",
              "      <td>137</td>\n",
              "      <td>165</td>\n",
              "      <td>80</td>\n",
              "      <td>134</td>\n",
              "      <td>163</td>\n",
              "      <td>75</td>\n",
              "      <td>139</td>\n",
              "      <td>164</td>\n",
              "      <td>72</td>\n",
              "      <td>138</td>\n",
              "      <td>161</td>\n",
              "      <td>70</td>\n",
              "      <td>138</td>\n",
              "      <td>161</td>\n",
              "      <td>70</td>\n",
              "      <td>139</td>\n",
              "      <td>162</td>\n",
              "      <td>73</td>\n",
              "      <td>134</td>\n",
              "      <td>159</td>\n",
              "      <td>67</td>\n",
              "      <td>133</td>\n",
              "      <td>158</td>\n",
              "      <td>66</td>\n",
              "      <td>134</td>\n",
              "      <td>159</td>\n",
              "      <td>66</td>\n",
              "      <td>131</td>\n",
              "      <td>157</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>83</td>\n",
              "      <td>85</td>\n",
              "      <td>64</td>\n",
              "      <td>82</td>\n",
              "      <td>84</td>\n",
              "      <td>63</td>\n",
              "      <td>91</td>\n",
              "      <td>92</td>\n",
              "      <td>71</td>\n",
              "      <td>93</td>\n",
              "      <td>91</td>\n",
              "      <td>67</td>\n",
              "      <td>94</td>\n",
              "      <td>92</td>\n",
              "      <td>64</td>\n",
              "      <td>156</td>\n",
              "      <td>145</td>\n",
              "      <td>94</td>\n",
              "      <td>215</td>\n",
              "      <td>181</td>\n",
              "      <td>122</td>\n",
              "      <td>109</td>\n",
              "      <td>71</td>\n",
              "      <td>39</td>\n",
              "      <td>116</td>\n",
              "      <td>98</td>\n",
              "      <td>71</td>\n",
              "      <td>132</td>\n",
              "      <td>119</td>\n",
              "      <td>76</td>\n",
              "      <td>127</td>\n",
              "      <td>116</td>\n",
              "      <td>73</td>\n",
              "      <td>106</td>\n",
              "      <td>96</td>\n",
              "      <td>64</td>\n",
              "      <td>108</td>\n",
              "      <td>102</td>\n",
              "      <td>75</td>\n",
              "      <td>103</td>\n",
              "      <td>...</td>\n",
              "      <td>54</td>\n",
              "      <td>147</td>\n",
              "      <td>108</td>\n",
              "      <td>55</td>\n",
              "      <td>119</td>\n",
              "      <td>80</td>\n",
              "      <td>34</td>\n",
              "      <td>111</td>\n",
              "      <td>72</td>\n",
              "      <td>29</td>\n",
              "      <td>120</td>\n",
              "      <td>76</td>\n",
              "      <td>30</td>\n",
              "      <td>120</td>\n",
              "      <td>80</td>\n",
              "      <td>36</td>\n",
              "      <td>108</td>\n",
              "      <td>72</td>\n",
              "      <td>34</td>\n",
              "      <td>101</td>\n",
              "      <td>67</td>\n",
              "      <td>30</td>\n",
              "      <td>103</td>\n",
              "      <td>71</td>\n",
              "      <td>31</td>\n",
              "      <td>99</td>\n",
              "      <td>70</td>\n",
              "      <td>33</td>\n",
              "      <td>89</td>\n",
              "      <td>62</td>\n",
              "      <td>30</td>\n",
              "      <td>78</td>\n",
              "      <td>55</td>\n",
              "      <td>24</td>\n",
              "      <td>78</td>\n",
              "      <td>54</td>\n",
              "      <td>24</td>\n",
              "      <td>74</td>\n",
              "      <td>49</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>243</td>\n",
              "      <td>244</td>\n",
              "      <td>245</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>254</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>250</td>\n",
              "      <td>252</td>\n",
              "      <td>254</td>\n",
              "      <td>247</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>248</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>248</td>\n",
              "      <td>254</td>\n",
              "      <td>252</td>\n",
              "      <td>248</td>\n",
              "      <td>254</td>\n",
              "      <td>252</td>\n",
              "      <td>249</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>236</td>\n",
              "      <td>237</td>\n",
              "      <td>240</td>\n",
              "      <td>229</td>\n",
              "      <td>230</td>\n",
              "      <td>235</td>\n",
              "      <td>238</td>\n",
              "      <td>...</td>\n",
              "      <td>233</td>\n",
              "      <td>247</td>\n",
              "      <td>247</td>\n",
              "      <td>245</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>250</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>71</td>\n",
              "      <td>58</td>\n",
              "      <td>42</td>\n",
              "      <td>71</td>\n",
              "      <td>58</td>\n",
              "      <td>42</td>\n",
              "      <td>60</td>\n",
              "      <td>47</td>\n",
              "      <td>32</td>\n",
              "      <td>79</td>\n",
              "      <td>59</td>\n",
              "      <td>45</td>\n",
              "      <td>77</td>\n",
              "      <td>54</td>\n",
              "      <td>41</td>\n",
              "      <td>72</td>\n",
              "      <td>50</td>\n",
              "      <td>37</td>\n",
              "      <td>78</td>\n",
              "      <td>58</td>\n",
              "      <td>43</td>\n",
              "      <td>77</td>\n",
              "      <td>58</td>\n",
              "      <td>41</td>\n",
              "      <td>83</td>\n",
              "      <td>64</td>\n",
              "      <td>47</td>\n",
              "      <td>81</td>\n",
              "      <td>61</td>\n",
              "      <td>46</td>\n",
              "      <td>75</td>\n",
              "      <td>53</td>\n",
              "      <td>40</td>\n",
              "      <td>81</td>\n",
              "      <td>59</td>\n",
              "      <td>47</td>\n",
              "      <td>105</td>\n",
              "      <td>84</td>\n",
              "      <td>71</td>\n",
              "      <td>121</td>\n",
              "      <td>...</td>\n",
              "      <td>105</td>\n",
              "      <td>146</td>\n",
              "      <td>140</td>\n",
              "      <td>107</td>\n",
              "      <td>110</td>\n",
              "      <td>104</td>\n",
              "      <td>72</td>\n",
              "      <td>128</td>\n",
              "      <td>122</td>\n",
              "      <td>90</td>\n",
              "      <td>148</td>\n",
              "      <td>140</td>\n",
              "      <td>104</td>\n",
              "      <td>133</td>\n",
              "      <td>123</td>\n",
              "      <td>83</td>\n",
              "      <td>138</td>\n",
              "      <td>128</td>\n",
              "      <td>89</td>\n",
              "      <td>144</td>\n",
              "      <td>137</td>\n",
              "      <td>99</td>\n",
              "      <td>136</td>\n",
              "      <td>132</td>\n",
              "      <td>99</td>\n",
              "      <td>113</td>\n",
              "      <td>109</td>\n",
              "      <td>79</td>\n",
              "      <td>89</td>\n",
              "      <td>81</td>\n",
              "      <td>55</td>\n",
              "      <td>97</td>\n",
              "      <td>80</td>\n",
              "      <td>58</td>\n",
              "      <td>115</td>\n",
              "      <td>103</td>\n",
              "      <td>79</td>\n",
              "      <td>122</td>\n",
              "      <td>114</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>98</td>\n",
              "      <td>79</td>\n",
              "      <td>36</td>\n",
              "      <td>97</td>\n",
              "      <td>78</td>\n",
              "      <td>32</td>\n",
              "      <td>98</td>\n",
              "      <td>80</td>\n",
              "      <td>34</td>\n",
              "      <td>93</td>\n",
              "      <td>75</td>\n",
              "      <td>36</td>\n",
              "      <td>94</td>\n",
              "      <td>69</td>\n",
              "      <td>36</td>\n",
              "      <td>88</td>\n",
              "      <td>67</td>\n",
              "      <td>49</td>\n",
              "      <td>94</td>\n",
              "      <td>86</td>\n",
              "      <td>86</td>\n",
              "      <td>101</td>\n",
              "      <td>84</td>\n",
              "      <td>79</td>\n",
              "      <td>53</td>\n",
              "      <td>49</td>\n",
              "      <td>46</td>\n",
              "      <td>51</td>\n",
              "      <td>34</td>\n",
              "      <td>38</td>\n",
              "      <td>103</td>\n",
              "      <td>44</td>\n",
              "      <td>48</td>\n",
              "      <td>110</td>\n",
              "      <td>65</td>\n",
              "      <td>63</td>\n",
              "      <td>115</td>\n",
              "      <td>88</td>\n",
              "      <td>72</td>\n",
              "      <td>121</td>\n",
              "      <td>...</td>\n",
              "      <td>70</td>\n",
              "      <td>148</td>\n",
              "      <td>116</td>\n",
              "      <td>62</td>\n",
              "      <td>144</td>\n",
              "      <td>112</td>\n",
              "      <td>57</td>\n",
              "      <td>152</td>\n",
              "      <td>120</td>\n",
              "      <td>63</td>\n",
              "      <td>151</td>\n",
              "      <td>120</td>\n",
              "      <td>61</td>\n",
              "      <td>148</td>\n",
              "      <td>114</td>\n",
              "      <td>55</td>\n",
              "      <td>147</td>\n",
              "      <td>111</td>\n",
              "      <td>54</td>\n",
              "      <td>147</td>\n",
              "      <td>109</td>\n",
              "      <td>58</td>\n",
              "      <td>143</td>\n",
              "      <td>102</td>\n",
              "      <td>56</td>\n",
              "      <td>135</td>\n",
              "      <td>94</td>\n",
              "      <td>51</td>\n",
              "      <td>133</td>\n",
              "      <td>94</td>\n",
              "      <td>52</td>\n",
              "      <td>129</td>\n",
              "      <td>89</td>\n",
              "      <td>47</td>\n",
              "      <td>128</td>\n",
              "      <td>89</td>\n",
              "      <td>53</td>\n",
              "      <td>107</td>\n",
              "      <td>78</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>177</td>\n",
              "      <td>198</td>\n",
              "      <td>109</td>\n",
              "      <td>180</td>\n",
              "      <td>203</td>\n",
              "      <td>108</td>\n",
              "      <td>189</td>\n",
              "      <td>212</td>\n",
              "      <td>119</td>\n",
              "      <td>191</td>\n",
              "      <td>212</td>\n",
              "      <td>135</td>\n",
              "      <td>185</td>\n",
              "      <td>205</td>\n",
              "      <td>141</td>\n",
              "      <td>171</td>\n",
              "      <td>189</td>\n",
              "      <td>136</td>\n",
              "      <td>138</td>\n",
              "      <td>151</td>\n",
              "      <td>115</td>\n",
              "      <td>98</td>\n",
              "      <td>106</td>\n",
              "      <td>86</td>\n",
              "      <td>67</td>\n",
              "      <td>68</td>\n",
              "      <td>61</td>\n",
              "      <td>59</td>\n",
              "      <td>56</td>\n",
              "      <td>57</td>\n",
              "      <td>61</td>\n",
              "      <td>58</td>\n",
              "      <td>61</td>\n",
              "      <td>66</td>\n",
              "      <td>63</td>\n",
              "      <td>70</td>\n",
              "      <td>73</td>\n",
              "      <td>71</td>\n",
              "      <td>83</td>\n",
              "      <td>87</td>\n",
              "      <td>...</td>\n",
              "      <td>122</td>\n",
              "      <td>129</td>\n",
              "      <td>138</td>\n",
              "      <td>122</td>\n",
              "      <td>108</td>\n",
              "      <td>115</td>\n",
              "      <td>101</td>\n",
              "      <td>99</td>\n",
              "      <td>104</td>\n",
              "      <td>91</td>\n",
              "      <td>101</td>\n",
              "      <td>103</td>\n",
              "      <td>91</td>\n",
              "      <td>101</td>\n",
              "      <td>104</td>\n",
              "      <td>85</td>\n",
              "      <td>100</td>\n",
              "      <td>106</td>\n",
              "      <td>85</td>\n",
              "      <td>103</td>\n",
              "      <td>113</td>\n",
              "      <td>92</td>\n",
              "      <td>117</td>\n",
              "      <td>130</td>\n",
              "      <td>112</td>\n",
              "      <td>104</td>\n",
              "      <td>121</td>\n",
              "      <td>101</td>\n",
              "      <td>86</td>\n",
              "      <td>106</td>\n",
              "      <td>85</td>\n",
              "      <td>69</td>\n",
              "      <td>87</td>\n",
              "      <td>69</td>\n",
              "      <td>56</td>\n",
              "      <td>73</td>\n",
              "      <td>58</td>\n",
              "      <td>39</td>\n",
              "      <td>52</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>37</td>\n",
              "      <td>66</td>\n",
              "      <td>24</td>\n",
              "      <td>45</td>\n",
              "      <td>70</td>\n",
              "      <td>32</td>\n",
              "      <td>45</td>\n",
              "      <td>68</td>\n",
              "      <td>23</td>\n",
              "      <td>33</td>\n",
              "      <td>53</td>\n",
              "      <td>14</td>\n",
              "      <td>33</td>\n",
              "      <td>48</td>\n",
              "      <td>20</td>\n",
              "      <td>38</td>\n",
              "      <td>51</td>\n",
              "      <td>22</td>\n",
              "      <td>37</td>\n",
              "      <td>51</td>\n",
              "      <td>18</td>\n",
              "      <td>37</td>\n",
              "      <td>51</td>\n",
              "      <td>16</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>25</td>\n",
              "      <td>38</td>\n",
              "      <td>52</td>\n",
              "      <td>15</td>\n",
              "      <td>72</td>\n",
              "      <td>86</td>\n",
              "      <td>47</td>\n",
              "      <td>132</td>\n",
              "      <td>147</td>\n",
              "      <td>107</td>\n",
              "      <td>127</td>\n",
              "      <td>143</td>\n",
              "      <td>105</td>\n",
              "      <td>124</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>140</td>\n",
              "      <td>135</td>\n",
              "      <td>101</td>\n",
              "      <td>209</td>\n",
              "      <td>199</td>\n",
              "      <td>161</td>\n",
              "      <td>189</td>\n",
              "      <td>179</td>\n",
              "      <td>149</td>\n",
              "      <td>217</td>\n",
              "      <td>211</td>\n",
              "      <td>185</td>\n",
              "      <td>192</td>\n",
              "      <td>189</td>\n",
              "      <td>160</td>\n",
              "      <td>142</td>\n",
              "      <td>136</td>\n",
              "      <td>102</td>\n",
              "      <td>132</td>\n",
              "      <td>121</td>\n",
              "      <td>82</td>\n",
              "      <td>123</td>\n",
              "      <td>110</td>\n",
              "      <td>67</td>\n",
              "      <td>167</td>\n",
              "      <td>154</td>\n",
              "      <td>111</td>\n",
              "      <td>69</td>\n",
              "      <td>57</td>\n",
              "      <td>29</td>\n",
              "      <td>28</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>31</td>\n",
              "      <td>25</td>\n",
              "      <td>12</td>\n",
              "      <td>28</td>\n",
              "      <td>25</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>131</td>\n",
              "      <td>188</td>\n",
              "      <td>220</td>\n",
              "      <td>130</td>\n",
              "      <td>192</td>\n",
              "      <td>228</td>\n",
              "      <td>134</td>\n",
              "      <td>188</td>\n",
              "      <td>231</td>\n",
              "      <td>137</td>\n",
              "      <td>188</td>\n",
              "      <td>233</td>\n",
              "      <td>134</td>\n",
              "      <td>190</td>\n",
              "      <td>233</td>\n",
              "      <td>133</td>\n",
              "      <td>189</td>\n",
              "      <td>235</td>\n",
              "      <td>135</td>\n",
              "      <td>188</td>\n",
              "      <td>235</td>\n",
              "      <td>136</td>\n",
              "      <td>188</td>\n",
              "      <td>226</td>\n",
              "      <td>114</td>\n",
              "      <td>167</td>\n",
              "      <td>186</td>\n",
              "      <td>92</td>\n",
              "      <td>145</td>\n",
              "      <td>157</td>\n",
              "      <td>99</td>\n",
              "      <td>152</td>\n",
              "      <td>177</td>\n",
              "      <td>118</td>\n",
              "      <td>173</td>\n",
              "      <td>203</td>\n",
              "      <td>126</td>\n",
              "      <td>180</td>\n",
              "      <td>214</td>\n",
              "      <td>89</td>\n",
              "      <td>...</td>\n",
              "      <td>181</td>\n",
              "      <td>190</td>\n",
              "      <td>200</td>\n",
              "      <td>162</td>\n",
              "      <td>77</td>\n",
              "      <td>123</td>\n",
              "      <td>76</td>\n",
              "      <td>33</td>\n",
              "      <td>98</td>\n",
              "      <td>54</td>\n",
              "      <td>163</td>\n",
              "      <td>192</td>\n",
              "      <td>179</td>\n",
              "      <td>201</td>\n",
              "      <td>218</td>\n",
              "      <td>201</td>\n",
              "      <td>180</td>\n",
              "      <td>203</td>\n",
              "      <td>185</td>\n",
              "      <td>90</td>\n",
              "      <td>145</td>\n",
              "      <td>109</td>\n",
              "      <td>43</td>\n",
              "      <td>116</td>\n",
              "      <td>52</td>\n",
              "      <td>75</td>\n",
              "      <td>142</td>\n",
              "      <td>68</td>\n",
              "      <td>88</td>\n",
              "      <td>149</td>\n",
              "      <td>66</td>\n",
              "      <td>84</td>\n",
              "      <td>147</td>\n",
              "      <td>59</td>\n",
              "      <td>78</td>\n",
              "      <td>145</td>\n",
              "      <td>57</td>\n",
              "      <td>82</td>\n",
              "      <td>144</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3072 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0     1     2     3     4     5     ...  3066  3067  3068  3069  3070  3071\n",
              "0       43    24     1    67    28     6  ...    15    10     8    18    11     3\n",
              "1       34    37    42    34    37    42  ...   179   183   182   178   182   181\n",
              "2      209   227   247   197   224   244  ...   134   159    66   131   157    65\n",
              "3       83    85    64    82    84    63  ...    78    54    24    74    49    21\n",
              "4      243   244   245   251   252   254  ...   253   253   251   253   253   251\n",
              "...    ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
              "1995    71    58    42    71    58    42  ...   115   103    79   122   114    88\n",
              "1996    98    79    36    97    78    32  ...   128    89    53   107    78    50\n",
              "1997   177   198   109   180   203   108  ...    56    73    58    39    52    40\n",
              "1998    37    66    24    45    70    32  ...    31    25    12    28    25    13\n",
              "1999   131   188   220   130   192   228  ...    78   145    57    82   144    63\n",
              "\n",
              "[2000 rows x 3072 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YY5y2IYZs2k",
        "outputId": "7781021f-4679-4b19-a69c-d84029a67cef"
      },
      "source": [
        "train_label = pd.read_csv('/content/train_label.csv')\n",
        "print(\"**************\")\n",
        "print(\"Unique lables:\",sorted(train_label['label'].unique()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************\n",
            "Unique lables: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "9go1jn5Irp9a",
        "outputId": "d13c116e-1410-4add-931e-df1d5f1f23bb"
      },
      "source": [
        "train_label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9995</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9996</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9997</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9998</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9999</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  label\n",
              "0        0     65\n",
              "1        1     10\n",
              "2        2     71\n",
              "3        3      0\n",
              "4        4     67\n",
              "...    ...    ...\n",
              "9995  9995     51\n",
              "9996  9996     74\n",
              "9997  9997      6\n",
              "9998  9998     39\n",
              "9999  9999     23\n",
              "\n",
              "[10000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZduTwD6r0XJ"
      },
      "source": [
        "# correct_train_label = pd.DataFrame(train_label['label'],columns=['label'])\n",
        "# correct_train_label = correct_train_label.iloc[1:,:]\n",
        "# correct_train_label = correct_train_label.reset_index(drop=True)\n",
        "# correct_train_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ufnmPnZ3NX"
      },
      "source": [
        "train_array = train_data.to_numpy()\n",
        "train_array_reshaped=np.reshape(train_array,(-1, 32, 32, 3))\n",
        "# print(train_array_reshaped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "ykYDdQo1fGMg",
        "outputId": "1c3f2a21-712f-43f4-872a-47efc935a927"
      },
      "source": [
        "plt.figure(figsize = (10,2))\n",
        "plt.imshow(train_array_reshaped[0].astype(int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0474835750>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-9oIY78g4Xe"
      },
      "source": [
        "test_array = test_data.to_numpy()\n",
        "test_array_reshaped=np.reshape(test_array,(-1, 32, 32, 3))\n",
        "# print(train_array_reshaped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "O4Z9tB_BNKZR",
        "outputId": "bbd1e0e9-0b27-4bb5-f3b1-843f64541da1"
      },
      "source": [
        "plt.figure(figsize = (15,2))\n",
        "plt.imshow(test_array_reshaped[-1].astype(int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f04749e41d0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ooHkrjIKHFM"
      },
      "source": [
        "#0.3900 Accuracy Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQXXBBVgivZg"
      },
      "source": [
        "# from keras.callbacks import LearningRateScheduler\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# X = train_array_reshaped\n",
        "# y = to_categorical(np.array(train_label['label']))\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=25)\n",
        "\n",
        "# X_train = X_train/255.0\n",
        "# X_val = X_val/255.0\n",
        "\n",
        "# def lr_schedule(epoch):\n",
        "#     lrate = 0.001\n",
        "#     if epoch > 50:\n",
        "#         lrate = 0.0005\n",
        "#     if epoch > 75:\n",
        "#         lrate = 0.0003\n",
        "#     if epoch > 90:\n",
        "#         lrate = 0.0001\n",
        "#     return lrate\n",
        "\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# from tensorflow.keras.regularizers import l2\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(64, (3,3), activation='relu',kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2,2)))\n",
        "# model.add(Dropout(0.3))\n",
        "    \n",
        "# model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(512, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "# model.compile(loss=keras.losses.categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
        "# model.summary()\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#         featurewise_center=False,  \n",
        "#         samplewise_center=False, \n",
        "#         featurewise_std_normalization=False,  \n",
        "#         samplewise_std_normalization=False,  \n",
        "#         zca_whitening=False,  \n",
        "#         rotation_range=20,  \n",
        "#         width_shift_range=0.1,  \n",
        "#         height_shift_range=0.1, \n",
        "#         horizontal_flip=True, \n",
        "#         vertical_flip=False\n",
        "#     )\n",
        "# datagen.fit(X_train)\n",
        "\n",
        "# history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 128), epochs=100, validation_data=(X_val, y_val),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "\n",
        "# scores = model.evaluate(X_val, y_val, verbose=0)\n",
        "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF11UHk2znux"
      },
      "source": [
        "#0.4833 Accuracy Kaggle: predictions_5.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2FAZca9KRsE"
      },
      "source": [
        "# from keras.callbacks import LearningRateScheduler\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# X = train_array_reshaped\n",
        "# y = to_categorical(np.array(train_label['label']))\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=15)\n",
        "\n",
        "# X_train = X_train/255.0\n",
        "# X_val = X_val/255.0\n",
        "\n",
        "# def lr_schedule(epoch):\n",
        "#     lrate = 0.001\n",
        "#     if epoch > 75:\n",
        "#         lrate = 0.0007\n",
        "#     if epoch > 100:\n",
        "#         lrate = 0.0005\n",
        "#     if epoch > 125:\n",
        "#         lrate = 0.0003\n",
        "#     if epoch > 150:\n",
        "#         lrate = 0.0001\n",
        "#     return lrate\n",
        "\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# from tensorflow.keras.regularizers import l2\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(64, (3,3), activation='elu',kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2,2)))\n",
        "# model.add(Dropout(0.3))\n",
        "    \n",
        "# model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(512, activation='elu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "# model.compile(loss=keras.losses.categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
        "# model.summary()\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#         featurewise_center=False,  \n",
        "#         samplewise_center=False, \n",
        "#         featurewise_std_normalization=False,  \n",
        "#         samplewise_std_normalization=False,  \n",
        "#         zca_whitening=False,  \n",
        "#         rotation_range=10,  \n",
        "#         width_shift_range=0.1,  \n",
        "#         height_shift_range=0.1, \n",
        "#         horizontal_flip=True, \n",
        "#         vertical_flip=False\n",
        "#     )\n",
        "# datagen.fit(X_train)\n",
        "\n",
        "# history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 256), epochs=160, validation_data=(X_val, y_val),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "\n",
        "# scores = model.evaluate(X_val, y_val, verbose=0)\n",
        "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# plt.plot(history.history['accuracy'], label='accuracy')\n",
        "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.ylim([0.5, 1])\n",
        "# plt.legend(loc='lower right')\n",
        "\n",
        "# val_loss, val_acc = model.evaluate(X_val, y_val, verbose=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8lOnnGqf6uC"
      },
      "source": [
        "## 0.4866 Accuracy Kaggle: predictions_6.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvSSnmyf6AOX"
      },
      "source": [
        "# from keras.callbacks import LearningRateScheduler\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# X = train_array_reshaped\n",
        "# y = to_categorical(np.array(train_label['label']))\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=15)\n",
        "\n",
        "# X_train = X_train/255.0\n",
        "# X_val = X_val/255.0\n",
        "\n",
        "# def lr_schedule(epoch):\n",
        "#     lrate = 0.001\n",
        "#     if epoch > 75:\n",
        "#         lrate = 0.0007\n",
        "#     if epoch > 100:\n",
        "#         lrate = 0.0005\n",
        "#     if epoch > 125:\n",
        "#         lrate = 0.0003\n",
        "#     if epoch > 150:\n",
        "#         lrate = 0.0001\n",
        "#     return lrate\n",
        "\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# from tensorflow.keras.regularizers import l2\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(64, (3,3), activation='elu',kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2,2)))\n",
        "# model.add(Dropout(0.3))\n",
        "    \n",
        "# model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(512, activation='elu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# model.add(Dense(256, activation='elu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.001)))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "# model.compile(loss=keras.losses.categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
        "# model.summary()\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#         featurewise_center=False,  \n",
        "#         samplewise_center=False, \n",
        "#         featurewise_std_normalization=False,  \n",
        "#         samplewise_std_normalization=False,  \n",
        "#         zca_whitening=False,  \n",
        "#         rotation_range=10,  \n",
        "#         width_shift_range=0.1,  \n",
        "#         height_shift_range=0.1, \n",
        "#         horizontal_flip=True, \n",
        "#         vertical_flip=False\n",
        "#     )\n",
        "# datagen.fit(X_train)\n",
        "\n",
        "# history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 256), epochs=200, validation_data=(X_val, y_val),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "\n",
        "# scores = model.evaluate(X_val, y_val, verbose=0)\n",
        "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# plt.plot(history.history['accuracy'], label='accuracy')\n",
        "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.ylim([0.5, 1])\n",
        "# plt.legend(loc='lower right')\n",
        "\n",
        "# val_loss, val_acc = model.evaluate(X_val, y_val, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcsISrFwtiAr"
      },
      "source": [
        "## 0.49333 Accuracy Kaggle: predictions_7.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r2CF27PrgGZu",
        "outputId": "6db51890-3d13-4f18-ba0c-45e015d3e906"
      },
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "X = train_array_reshaped\n",
        "y = to_categorical(np.array(train_label['label']))\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=15)\n",
        "\n",
        "X_train = X_train/255.0\n",
        "X_val = X_val/255.0\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0009\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0007\n",
        "    if epoch > 125:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 150:\n",
        "        lrate = 0.0003\n",
        "    if epoch > 175:\n",
        "        lrate = 0.0001\n",
        "    if epoch > 190:\n",
        "        lrate = 0.00005\n",
        "    return lrate\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001), input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), activation='elu',kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "    \n",
        "model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3, 3), activation='elu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='elu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(256, activation='elu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.6))\n",
        "\n",
        "model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  \n",
        "        samplewise_center=False, \n",
        "        featurewise_std_normalization=False,  \n",
        "        samplewise_std_normalization=False,  \n",
        "        zca_whitening=False,  \n",
        "        rotation_range=10,  \n",
        "        width_shift_range=0.1,  \n",
        "        height_shift_range=0.1, \n",
        "        horizontal_flip=True, \n",
        "        vertical_flip=False\n",
        "    )\n",
        "datagen.fit(X_train)\n",
        "\n",
        "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 256), epochs=200, validation_data=(X_val, y_val),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "\n",
        "scores = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 4, 4, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               25700     \n",
            "=================================================================\n",
            "Total params: 11,993,444\n",
            "Trainable params: 11,984,612\n",
            "Non-trainable params: 8,832\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "36/36 [==============================] - 47s 330ms/step - loss: 14.6804 - accuracy: 0.0189 - val_loss: 486.0959 - val_accuracy: 0.0070\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 10s 284ms/step - loss: 13.8845 - accuracy: 0.0367 - val_loss: 62.0699 - val_accuracy: 0.0130\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 10s 285ms/step - loss: 13.1788 - accuracy: 0.0499 - val_loss: 19.2709 - val_accuracy: 0.0330\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 10s 287ms/step - loss: 12.5037 - accuracy: 0.0562 - val_loss: 12.0281 - val_accuracy: 0.0700\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 11.8100 - accuracy: 0.0667 - val_loss: 11.4137 - val_accuracy: 0.0710\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 11.1852 - accuracy: 0.0704 - val_loss: 10.9273 - val_accuracy: 0.0550\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 10.5559 - accuracy: 0.0848 - val_loss: 10.3365 - val_accuracy: 0.0560\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 11s 294ms/step - loss: 9.9369 - accuracy: 0.0852 - val_loss: 9.8409 - val_accuracy: 0.0630\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 9.3748 - accuracy: 0.0961 - val_loss: 9.3416 - val_accuracy: 0.0690\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 8.8461 - accuracy: 0.1038 - val_loss: 8.8903 - val_accuracy: 0.0660\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 8.3206 - accuracy: 0.1106 - val_loss: 8.0983 - val_accuracy: 0.1230\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 7.9131 - accuracy: 0.1179 - val_loss: 7.8723 - val_accuracy: 0.0890\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 7.4892 - accuracy: 0.1182 - val_loss: 7.7572 - val_accuracy: 0.0700\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 7.1060 - accuracy: 0.1298 - val_loss: 7.2328 - val_accuracy: 0.1050\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 6.7556 - accuracy: 0.1384 - val_loss: 6.6615 - val_accuracy: 0.1310\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 6.3948 - accuracy: 0.1556 - val_loss: 6.7554 - val_accuracy: 0.0770\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 6.1730 - accuracy: 0.1498 - val_loss: 6.7868 - val_accuracy: 0.1030\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 5.8740 - accuracy: 0.1577 - val_loss: 5.8929 - val_accuracy: 0.1580\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 5.6094 - accuracy: 0.1646 - val_loss: 5.7492 - val_accuracy: 0.1360\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 5.4096 - accuracy: 0.1714 - val_loss: 5.6985 - val_accuracy: 0.1400\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 5.1886 - accuracy: 0.1870 - val_loss: 5.6654 - val_accuracy: 0.1110\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 5.0654 - accuracy: 0.1838 - val_loss: 5.2630 - val_accuracy: 0.1650\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 4.9639 - accuracy: 0.1840 - val_loss: 5.2531 - val_accuracy: 0.1690\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 4.7655 - accuracy: 0.2044 - val_loss: 5.1423 - val_accuracy: 0.1650\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 4.6670 - accuracy: 0.2008 - val_loss: 5.2394 - val_accuracy: 0.1380\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 4.4889 - accuracy: 0.2110 - val_loss: 4.8993 - val_accuracy: 0.1570\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 4.4300 - accuracy: 0.2129 - val_loss: 4.6593 - val_accuracy: 0.1750\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 4.2836 - accuracy: 0.2224 - val_loss: 4.4045 - val_accuracy: 0.2320\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 4.2301 - accuracy: 0.2386 - val_loss: 4.6188 - val_accuracy: 0.1690\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 4.2598 - accuracy: 0.2182 - val_loss: 4.5865 - val_accuracy: 0.1870\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 4.1282 - accuracy: 0.2333 - val_loss: 4.5855 - val_accuracy: 0.1870\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 4.0391 - accuracy: 0.2398 - val_loss: 4.3075 - val_accuracy: 0.1990\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 4.0098 - accuracy: 0.2422 - val_loss: 4.2323 - val_accuracy: 0.2170\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 3.9417 - accuracy: 0.2447 - val_loss: 4.1598 - val_accuracy: 0.2140\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.9051 - accuracy: 0.2504 - val_loss: 4.1300 - val_accuracy: 0.2350\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 3.8839 - accuracy: 0.2526 - val_loss: 4.4658 - val_accuracy: 0.1990\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 11s 295ms/step - loss: 3.8382 - accuracy: 0.2579 - val_loss: 4.5281 - val_accuracy: 0.1750\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 3.8149 - accuracy: 0.2694 - val_loss: 4.0696 - val_accuracy: 0.2250\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.7637 - accuracy: 0.2729 - val_loss: 3.8637 - val_accuracy: 0.2540\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.7780 - accuracy: 0.2664 - val_loss: 4.0301 - val_accuracy: 0.2480\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.6676 - accuracy: 0.2822 - val_loss: 3.8300 - val_accuracy: 0.2680\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.6662 - accuracy: 0.2800 - val_loss: 4.0603 - val_accuracy: 0.2120\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.6456 - accuracy: 0.2842 - val_loss: 3.9752 - val_accuracy: 0.2590\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.6363 - accuracy: 0.2958 - val_loss: 4.0327 - val_accuracy: 0.2320\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.6185 - accuracy: 0.2858 - val_loss: 3.8876 - val_accuracy: 0.2580\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.5770 - accuracy: 0.3027 - val_loss: 3.7364 - val_accuracy: 0.2810\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 3.5051 - accuracy: 0.3050 - val_loss: 4.0836 - val_accuracy: 0.2380\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.6307 - accuracy: 0.2853 - val_loss: 3.8799 - val_accuracy: 0.2700\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.4716 - accuracy: 0.3280 - val_loss: 4.0317 - val_accuracy: 0.2190\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.5879 - accuracy: 0.3011 - val_loss: 3.6923 - val_accuracy: 0.2900\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.5280 - accuracy: 0.3220 - val_loss: 3.6311 - val_accuracy: 0.3130\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 3.4675 - accuracy: 0.3286 - val_loss: 3.9102 - val_accuracy: 0.2510\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.6063 - accuracy: 0.2987 - val_loss: 3.9107 - val_accuracy: 0.2420\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 3.4133 - accuracy: 0.3432 - val_loss: 3.9503 - val_accuracy: 0.2820\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 3.5184 - accuracy: 0.3182 - val_loss: 3.9771 - val_accuracy: 0.2680\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 3.4637 - accuracy: 0.3338 - val_loss: 3.9458 - val_accuracy: 0.2480\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 3.4489 - accuracy: 0.3469 - val_loss: 4.0251 - val_accuracy: 0.2380\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 11s 288ms/step - loss: 3.5332 - accuracy: 0.3192 - val_loss: 3.8087 - val_accuracy: 0.2880\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 3.4262 - accuracy: 0.3417 - val_loss: 3.8324 - val_accuracy: 0.2730\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.3513 - accuracy: 0.3546 - val_loss: 3.8523 - val_accuracy: 0.2490\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 11s 288ms/step - loss: 3.4831 - accuracy: 0.3376 - val_loss: 4.0553 - val_accuracy: 0.2640\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 10s 287ms/step - loss: 3.3733 - accuracy: 0.3631 - val_loss: 3.7539 - val_accuracy: 0.3100\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.5180 - accuracy: 0.3394 - val_loss: 3.8169 - val_accuracy: 0.3110\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.4242 - accuracy: 0.3620 - val_loss: 3.8302 - val_accuracy: 0.2990\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 3.3743 - accuracy: 0.3641 - val_loss: 3.7711 - val_accuracy: 0.2740\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 3.3317 - accuracy: 0.3724 - val_loss: 3.7184 - val_accuracy: 0.2900\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 3.2868 - accuracy: 0.3792 - val_loss: 3.9207 - val_accuracy: 0.2890\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.3812 - accuracy: 0.3640 - val_loss: 4.0072 - val_accuracy: 0.2580\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.3422 - accuracy: 0.3793 - val_loss: 4.0703 - val_accuracy: 0.2870\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 3.3537 - accuracy: 0.3769 - val_loss: 4.1467 - val_accuracy: 0.2590\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.3501 - accuracy: 0.3774 - val_loss: 3.7980 - val_accuracy: 0.3150\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.3325 - accuracy: 0.3846 - val_loss: 3.9970 - val_accuracy: 0.2700\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 3.3796 - accuracy: 0.3754 - val_loss: 4.4934 - val_accuracy: 0.2630\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 11s 306ms/step - loss: 3.3310 - accuracy: 0.3904 - val_loss: 4.1597 - val_accuracy: 0.2620\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 3.3867 - accuracy: 0.3800 - val_loss: 3.9807 - val_accuracy: 0.2920\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.2737 - accuracy: 0.4060 - val_loss: 3.9763 - val_accuracy: 0.2970\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.3185 - accuracy: 0.3918 - val_loss: 4.1277 - val_accuracy: 0.2860\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.2473 - accuracy: 0.4153 - val_loss: 3.5934 - val_accuracy: 0.3610\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.1476 - accuracy: 0.4321 - val_loss: 3.8201 - val_accuracy: 0.3240\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.2966 - accuracy: 0.3999 - val_loss: 3.8657 - val_accuracy: 0.3130\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.1851 - accuracy: 0.4296 - val_loss: 3.7083 - val_accuracy: 0.3530\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.1769 - accuracy: 0.4333 - val_loss: 3.7436 - val_accuracy: 0.3150\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.1861 - accuracy: 0.4276 - val_loss: 4.2338 - val_accuracy: 0.2670\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.2612 - accuracy: 0.4139 - val_loss: 3.7593 - val_accuracy: 0.3490\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.2524 - accuracy: 0.4249 - val_loss: 3.8328 - val_accuracy: 0.3410\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 3.2793 - accuracy: 0.4207 - val_loss: 3.8555 - val_accuracy: 0.3160\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 3.1774 - accuracy: 0.4402 - val_loss: 3.8643 - val_accuracy: 0.3440\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 3.1237 - accuracy: 0.4606 - val_loss: 3.7950 - val_accuracy: 0.3330\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.0888 - accuracy: 0.4600 - val_loss: 3.9461 - val_accuracy: 0.3240\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.2745 - accuracy: 0.4226 - val_loss: 3.8039 - val_accuracy: 0.3940\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 3.1306 - accuracy: 0.4610 - val_loss: 3.9305 - val_accuracy: 0.3280\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 3.1909 - accuracy: 0.4504 - val_loss: 4.7626 - val_accuracy: 0.2680\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 11s 297ms/step - loss: 3.2863 - accuracy: 0.4364 - val_loss: 3.8330 - val_accuracy: 0.3840\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.1468 - accuracy: 0.4651 - val_loss: 4.1410 - val_accuracy: 0.3160\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.2817 - accuracy: 0.4387 - val_loss: 3.8644 - val_accuracy: 0.3600\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 3.1304 - accuracy: 0.4729 - val_loss: 4.2099 - val_accuracy: 0.3110\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 11s 295ms/step - loss: 3.1419 - accuracy: 0.4610 - val_loss: 3.8195 - val_accuracy: 0.3650\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 3.0328 - accuracy: 0.4906 - val_loss: 3.7279 - val_accuracy: 0.3720\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 3.0918 - accuracy: 0.4750 - val_loss: 3.9862 - val_accuracy: 0.3310\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 3.1244 - accuracy: 0.4781 - val_loss: 4.1545 - val_accuracy: 0.3330\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 10s 285ms/step - loss: 3.0236 - accuracy: 0.4949 - val_loss: 3.6658 - val_accuracy: 0.3970\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 2.9921 - accuracy: 0.5032 - val_loss: 3.7081 - val_accuracy: 0.3710\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 2.9222 - accuracy: 0.5180 - val_loss: 3.7141 - val_accuracy: 0.3760\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 2.8298 - accuracy: 0.5367 - val_loss: 3.5966 - val_accuracy: 0.3940\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 2.9227 - accuracy: 0.5207 - val_loss: 3.6747 - val_accuracy: 0.4070\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.8145 - accuracy: 0.5348 - val_loss: 3.5576 - val_accuracy: 0.3930\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.7824 - accuracy: 0.5399 - val_loss: 3.5978 - val_accuracy: 0.3960\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 2.7756 - accuracy: 0.5494 - val_loss: 3.6226 - val_accuracy: 0.4100\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 2.8635 - accuracy: 0.5292 - val_loss: 3.6887 - val_accuracy: 0.3920\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 10s 287ms/step - loss: 2.7054 - accuracy: 0.5722 - val_loss: 3.7908 - val_accuracy: 0.3890\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 2.9854 - accuracy: 0.5028 - val_loss: 3.9050 - val_accuracy: 0.4160\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 2.7275 - accuracy: 0.5677 - val_loss: 3.6538 - val_accuracy: 0.4030\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 2.8450 - accuracy: 0.5384 - val_loss: 3.8517 - val_accuracy: 0.4000\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.8265 - accuracy: 0.5503 - val_loss: 3.8936 - val_accuracy: 0.4040\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 2.7381 - accuracy: 0.5680 - val_loss: 3.7445 - val_accuracy: 0.4000\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 2.7491 - accuracy: 0.5650 - val_loss: 3.5140 - val_accuracy: 0.4350\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.7182 - accuracy: 0.5778 - val_loss: 3.7958 - val_accuracy: 0.3850\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.6442 - accuracy: 0.5936 - val_loss: 3.6617 - val_accuracy: 0.3910\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 2.7706 - accuracy: 0.5651 - val_loss: 3.7434 - val_accuracy: 0.4200\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.7189 - accuracy: 0.5708 - val_loss: 3.7878 - val_accuracy: 0.3960\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.7799 - accuracy: 0.5652 - val_loss: 3.9625 - val_accuracy: 0.3800\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.7257 - accuracy: 0.5829 - val_loss: 4.2148 - val_accuracy: 0.3630\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 2.6469 - accuracy: 0.6013 - val_loss: 3.7629 - val_accuracy: 0.4100\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 2.5974 - accuracy: 0.6077 - val_loss: 4.3010 - val_accuracy: 0.3420\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 2.7706 - accuracy: 0.5682 - val_loss: 4.3090 - val_accuracy: 0.3660\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.6086 - accuracy: 0.6179 - val_loss: 4.0604 - val_accuracy: 0.3720\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.5066 - accuracy: 0.6360 - val_loss: 3.4046 - val_accuracy: 0.4670\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 10s 294ms/step - loss: 2.4069 - accuracy: 0.6558 - val_loss: 3.5505 - val_accuracy: 0.4320\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.3475 - accuracy: 0.6627 - val_loss: 3.3294 - val_accuracy: 0.4550\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 2.3150 - accuracy: 0.6706 - val_loss: 3.8540 - val_accuracy: 0.3750\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.3548 - accuracy: 0.6593 - val_loss: 3.6577 - val_accuracy: 0.4250\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 2.2821 - accuracy: 0.6756 - val_loss: 3.7379 - val_accuracy: 0.4100\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 11s 294ms/step - loss: 2.3708 - accuracy: 0.6544 - val_loss: 3.6127 - val_accuracy: 0.4340\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 2.2970 - accuracy: 0.6810 - val_loss: 3.6607 - val_accuracy: 0.4510\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.2897 - accuracy: 0.6714 - val_loss: 3.5695 - val_accuracy: 0.4380\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 2.1986 - accuracy: 0.7020 - val_loss: 3.5318 - val_accuracy: 0.4360\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.1916 - accuracy: 0.6940 - val_loss: 3.8382 - val_accuracy: 0.3780\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 2.2561 - accuracy: 0.6782 - val_loss: 3.7035 - val_accuracy: 0.4580\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 2.2023 - accuracy: 0.7007 - val_loss: 3.7691 - val_accuracy: 0.4290\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.2064 - accuracy: 0.6947 - val_loss: 3.8756 - val_accuracy: 0.4190\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 2.2406 - accuracy: 0.6922 - val_loss: 3.9261 - val_accuracy: 0.4030\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 2.2753 - accuracy: 0.6854 - val_loss: 3.9268 - val_accuracy: 0.4340\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 10s 294ms/step - loss: 2.2397 - accuracy: 0.7022 - val_loss: 3.8989 - val_accuracy: 0.4170\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 2.2056 - accuracy: 0.7036 - val_loss: 3.9041 - val_accuracy: 0.4130\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 2.1066 - accuracy: 0.7266 - val_loss: 3.8426 - val_accuracy: 0.4130\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 2.1034 - accuracy: 0.7253 - val_loss: 3.8338 - val_accuracy: 0.4360\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 2.1054 - accuracy: 0.7282 - val_loss: 3.6855 - val_accuracy: 0.4420\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.1268 - accuracy: 0.7216 - val_loss: 3.6141 - val_accuracy: 0.4390\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.0313 - accuracy: 0.7448 - val_loss: 4.1161 - val_accuracy: 0.4000\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 2.1182 - accuracy: 0.7267 - val_loss: 4.0054 - val_accuracy: 0.4190\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 2.1749 - accuracy: 0.7116 - val_loss: 3.9211 - val_accuracy: 0.4280\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 1.9557 - accuracy: 0.7708 - val_loss: 3.6150 - val_accuracy: 0.4620\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 1.8942 - accuracy: 0.7830 - val_loss: 3.5617 - val_accuracy: 0.4640\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 11s 311ms/step - loss: 1.8379 - accuracy: 0.7944 - val_loss: 3.5037 - val_accuracy: 0.4770\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 1.7510 - accuracy: 0.8142 - val_loss: 3.4557 - val_accuracy: 0.4780\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.7220 - accuracy: 0.8181 - val_loss: 3.3882 - val_accuracy: 0.4750\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 1.6369 - accuracy: 0.8361 - val_loss: 3.5244 - val_accuracy: 0.4630\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.6793 - accuracy: 0.8194 - val_loss: 3.4752 - val_accuracy: 0.4750\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.6323 - accuracy: 0.8349 - val_loss: 3.6671 - val_accuracy: 0.4350\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 1.6252 - accuracy: 0.8299 - val_loss: 3.4354 - val_accuracy: 0.4790\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.6254 - accuracy: 0.8292 - val_loss: 3.4955 - val_accuracy: 0.4780\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.5599 - accuracy: 0.8438 - val_loss: 3.6590 - val_accuracy: 0.4610\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 1.6674 - accuracy: 0.8106 - val_loss: 3.6838 - val_accuracy: 0.4760\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.6137 - accuracy: 0.8319 - val_loss: 3.5701 - val_accuracy: 0.4850\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 1.5719 - accuracy: 0.8397 - val_loss: 3.6236 - val_accuracy: 0.4750\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.5188 - accuracy: 0.8530 - val_loss: 3.7528 - val_accuracy: 0.4540\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.5397 - accuracy: 0.8438 - val_loss: 3.7886 - val_accuracy: 0.4480\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.5362 - accuracy: 0.8393 - val_loss: 3.8852 - val_accuracy: 0.4650\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.5794 - accuracy: 0.8333 - val_loss: 3.8501 - val_accuracy: 0.4280\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.5547 - accuracy: 0.8422 - val_loss: 3.8059 - val_accuracy: 0.4650\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 1.5256 - accuracy: 0.8490 - val_loss: 3.7367 - val_accuracy: 0.4710\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.5280 - accuracy: 0.8474 - val_loss: 3.6005 - val_accuracy: 0.4810\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.5160 - accuracy: 0.8539 - val_loss: 3.8381 - val_accuracy: 0.4720\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 1.5018 - accuracy: 0.8550 - val_loss: 3.6653 - val_accuracy: 0.4820\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 10s 296ms/step - loss: 1.4834 - accuracy: 0.8564 - val_loss: 3.5604 - val_accuracy: 0.5100\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 11s 297ms/step - loss: 1.4698 - accuracy: 0.8589 - val_loss: 3.5783 - val_accuracy: 0.4850\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 1.3330 - accuracy: 0.8982 - val_loss: 3.4343 - val_accuracy: 0.5100\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 10s 288ms/step - loss: 1.2888 - accuracy: 0.9088 - val_loss: 3.4326 - val_accuracy: 0.5120\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.2563 - accuracy: 0.9194 - val_loss: 3.4488 - val_accuracy: 0.5200\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 1.2392 - accuracy: 0.9206 - val_loss: 3.4882 - val_accuracy: 0.5020\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.2252 - accuracy: 0.9231 - val_loss: 3.4329 - val_accuracy: 0.5090\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.2024 - accuracy: 0.9239 - val_loss: 3.4937 - val_accuracy: 0.4960\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.1806 - accuracy: 0.9307 - val_loss: 3.3850 - val_accuracy: 0.5060\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 1.1618 - accuracy: 0.9339 - val_loss: 3.4078 - val_accuracy: 0.5080\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 10s 286ms/step - loss: 1.1454 - accuracy: 0.9364 - val_loss: 3.4238 - val_accuracy: 0.5050\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 1.1354 - accuracy: 0.9391 - val_loss: 3.4248 - val_accuracy: 0.5110\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 1.1401 - accuracy: 0.9341 - val_loss: 3.5066 - val_accuracy: 0.4970\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 1.1110 - accuracy: 0.9400 - val_loss: 3.4000 - val_accuracy: 0.5080\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.1065 - accuracy: 0.9377 - val_loss: 3.3756 - val_accuracy: 0.4990\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.0913 - accuracy: 0.9406 - val_loss: 3.4009 - val_accuracy: 0.5000\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.0712 - accuracy: 0.9462 - val_loss: 3.4309 - val_accuracy: 0.5060\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 1.0574 - accuracy: 0.9501 - val_loss: 3.4047 - val_accuracy: 0.5110\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.0461 - accuracy: 0.9531 - val_loss: 3.3704 - val_accuracy: 0.5100\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 11s 289ms/step - loss: 1.0376 - accuracy: 0.9528 - val_loss: 3.4263 - val_accuracy: 0.5100\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 11s 291ms/step - loss: 1.0235 - accuracy: 0.9599 - val_loss: 3.4075 - val_accuracy: 0.5170\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 1.0262 - accuracy: 0.9580 - val_loss: 3.4482 - val_accuracy: 0.5180\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 1.0136 - accuracy: 0.9596 - val_loss: 3.4611 - val_accuracy: 0.5170\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 11s 293ms/step - loss: 1.0075 - accuracy: 0.9591 - val_loss: 3.4815 - val_accuracy: 0.5070\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 11s 292ms/step - loss: 1.0007 - accuracy: 0.9618 - val_loss: 3.5044 - val_accuracy: 0.5040\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 11s 290ms/step - loss: 0.9972 - accuracy: 0.9614 - val_loss: 3.4330 - val_accuracy: 0.5080\n",
            "Accuracy: 50.80%\n",
            "32/32 - 1s - loss: 3.4330 - accuracy: 0.5080\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx_gYUbWj5tU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "outputId": "552c3c8f-2cf6-4369-d93e-4b8058469d43"
      },
      "source": [
        "test_array_reshaped = test_array_reshaped/255.0\n",
        "predictions = model.predict(test_array_reshaped)\n",
        "print(predictions[1])\n",
        "print(len(predictions))\n",
        "predictions_list = []\n",
        "for i in range(len(predictions)):\n",
        "  predictions_list.append(np.argmax(predictions[i]))\n",
        "# plt.figure(figsize = (15,2))\n",
        "# plt.imshow((test_array_reshaped[0]*255).astype(int))\n",
        "labels_id = []\n",
        "for i in range(0,2000):\n",
        "  labels_id.append(i)\n",
        "labels_id_df = pd.DataFrame(labels_id,columns=['id'])\n",
        "labels_id_df  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.06118508e-04 3.89422802e-03 3.21070454e-03 1.00405146e-06\n",
            " 6.74484727e-06 2.81941086e-01 5.35888239e-06 7.78782778e-05\n",
            " 4.35729562e-05 1.56067021e-03 1.69246539e-01 1.64420903e-03\n",
            " 3.60471779e-04 1.77113689e-04 1.57390925e-04 1.24079452e-04\n",
            " 2.78819294e-04 1.90209789e-06 8.45678849e-04 7.76552315e-06\n",
            " 3.54498141e-02 4.33782077e-07 1.82273181e-03 1.20938486e-04\n",
            " 4.20828396e-03 1.28338501e-01 3.78937577e-03 1.89155445e-03\n",
            " 5.88361407e-04 2.55257124e-04 5.29151948e-05 2.28661111e-06\n",
            " 4.73622084e-02 1.25251943e-04 6.88372438e-06 1.98174428e-04\n",
            " 4.76784589e-05 1.21060577e-04 1.81333089e-05 2.67000613e-03\n",
            " 1.05152689e-01 5.00685128e-04 5.16182354e-07 1.54426573e-07\n",
            " 2.06744559e-02 2.20924313e-03 5.57321240e-04 2.54802417e-05\n",
            " 1.42324952e-05 4.38067000e-05 4.51186585e-04 1.85109835e-04\n",
            " 1.55358441e-06 7.20338721e-05 4.38147617e-05 5.49168675e-04\n",
            " 7.03384103e-06 2.27727974e-03 1.30922790e-03 6.05009154e-06\n",
            " 8.00882190e-05 1.82707154e-03 3.30812713e-06 7.41869553e-06\n",
            " 7.84264193e-06 3.28889955e-03 1.46607124e-06 9.99846961e-03\n",
            " 1.67033082e-04 6.71022455e-04 1.76728165e-04 1.23313384e-03\n",
            " 3.30804396e-05 1.53055554e-03 1.77627502e-04 2.21621216e-04\n",
            " 6.64322351e-06 1.18517631e-03 1.27552135e-04 1.91663421e-04\n",
            " 8.47671181e-05 9.62553750e-05 2.10320167e-07 4.76169771e-05\n",
            " 2.08040662e-02 1.53456919e-03 2.39333254e-03 1.58761116e-03\n",
            " 1.63817595e-06 3.02314220e-05 9.99347685e-05 1.19166784e-01\n",
            " 3.74221854e-05 5.93472784e-03 2.83508125e-05 3.26430396e-04\n",
            " 2.49585310e-06 2.56512408e-06 1.28995089e-04 1.81339495e-03]\n",
            "2000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id\n",
              "0        0\n",
              "1        1\n",
              "2        2\n",
              "3        3\n",
              "4        4\n",
              "...    ...\n",
              "1995  1995\n",
              "1996  1996\n",
              "1997  1997\n",
              "1998  1998\n",
              "1999  1999\n",
              "\n",
              "[2000 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV0ZQADK4DQc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "42acbca6-6c9f-4fbf-8863-612682517e2b"
      },
      "source": [
        "y_pred_df = pd.DataFrame(predictions_list) \n",
        "final_labels_df = [labels_id_df['id'],y_pred_df[0]]\n",
        "headers = ['id','label']\n",
        "final_prediction_df = pd.concat(final_labels_df, axis=1, keys=headers)\n",
        "final_prediction_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1995</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1996</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1997</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1998</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1999</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  label\n",
              "0        0     77\n",
              "1        1      5\n",
              "2        2     17\n",
              "3        3     43\n",
              "4        4     20\n",
              "...    ...    ...\n",
              "1995  1995      4\n",
              "1996  1996      8\n",
              "1997  1997     42\n",
              "1998  1998      1\n",
              "1999  1999     52\n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6zBx11b6DaJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "125d8886-9ee8-4495-98b2-8177d210229d"
      },
      "source": [
        "plt.figure(figsize = (15,2))\n",
        "plt.imshow((test_array_reshaped[-1]*255).astype(int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0470180f50>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je4WUMyZW-0h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "5a0ae736-6b16-48af-bc27-9fc2809ce590"
      },
      "source": [
        "plt.figure(figsize = (15,2))\n",
        "plt.imshow((test_array_reshaped[0]*255).astype(int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0470076d50>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcZjz0Dv_ep_"
      },
      "source": [
        "# final_prediction_df.to_csv('predictions_7.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-jo9wRJfPIK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}